{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3. Supervised Fraud Detection: Gradient Boosting\n",
                "\n",
                "Learns from known fraud cases (Class = 1).\n",
                "\n",
                "Tasks:\n",
                "1. Load data (with Anomaly scores).\n",
                "2. Split Train/Test.\n",
                "3. Train Gradient Boosting Classifier.\n",
                "4. Generate Fraud Probabilities."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.ensemble import GradientBoostingClassifier\n",
                "from sklearn.metrics import classification_report\n",
                "import pickle\n",
                "\n",
                "INPUT_FILE = '../data/cleaned_transactions_with_iso.csv'\n",
                "OUTPUT_FILE = '../data/transactions_with_predictions.csv'\n",
                "MODEL_FILE = '../outputs/gb_model.pkl'\n",
                "\n",
                "df = pd.read_csv(INPUT_FILE)\n",
                "print(f\"Data loaded. Shape: {df.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare Data\n",
                "X = df.drop(columns=['Class', 'Time', 'UserID']) # Use all features including anomaly_score if we want stacking, or exclude it to keep independent\n",
                "# The plan says \"Combine... df['fraud_probability'] + df['anomaly_score']\", implying they are generated independently or sequentially.\n",
                "# Often we use the unsupervised score as a feature in supervised learning. Let's include 'anomaly_score' as a feature.\n",
                "# BUT, the user prompt implies 'Hybrid Fraud Score' is a weighted average at the end. So strictly speaking we should probably NOT use anomaly_score as input here to follow the formula precisely? \n",
                "# Actually, using it as a feature is smarter. But let's follow the user's explicit Formula step later (Step 6) which implies they are two separate signals combined at the end. \n",
                "# So I will EXCLUDE anomaly_score from the supervised training features to keep them as distinct signals for the hybrid score.\n",
                "X = X.drop(columns=['anomaly_score'], errors='ignore') \n",
                "\n",
                "y = df['Class']\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
                "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Gradient Boosting\n",
                "gb = GradientBoostingClassifier(\n",
                "    n_estimators=300,\n",
                "    learning_rate=0.05,\n",
                "    max_depth=5,\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "print(\"Training Gradient Boosting...\")\n",
                "gb.fit(X_train, y_train)\n",
                "\n",
                "# Predict Probabilities for ALL data (for the final output CSV)\n",
                "# Ideally we only predict on Test for Evaluation, but for the 'Output' file we probably want scores for the whole dataset (or just the test set).\n",
                "# We'll predict for the whole dataset to create the full 'fraud_predictions.csv'\n",
                "all_probs = gb.predict_proba(X)[:, 1]\n",
                "df['fraud_probability'] = all_probs\n",
                "\n",
                "print(\"Training Complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save Model and Data\n",
                "with open(MODEL_FILE, 'wb') as f:\n",
                "    pickle.dump(gb, f)\n",
                "\n",
                "df.to_csv(OUTPUT_FILE, index=False)\n",
                "print(f\"Predictions saved to {OUTPUT_FILE}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}