{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 1. Data Preprocessing & Feature Engineering\n",
                "\n",
                "In this notebook, we will:\n",
                "1. Load the raw transaction data.\n",
                "2. Downsample to 50K records for performance.\n",
                "3. Engineer features: `Amount_log`, `Txn_per_hour` (Velocity).\n",
                "4. Export the cleaned dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "\n",
                "# Configuration\n",
                "INPUT_FILE = '../data/raw_transactions.csv'\n",
                "OUTPUT_FILE = '../data/cleaned_transactions.csv'\n",
                "DOWNSAMPLE_SIZE = 50000\n",
                "\n",
                "# Check if data exists, if not, warn user (or rely on synthetic generator which should have been run)\n",
                "if not os.path.exists(INPUT_FILE):\n",
                "    print(f\"Data file {INPUT_FILE} not found. Please run generate_data.py or place the dataset in the data folder.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Load Data\n",
                "try:\n",
                "    df = pd.read_csv(INPUT_FILE)\n",
                "    print(f\"Data loaded. Shape: {df.shape}\")\n",
                "except FileNotFoundError:\n",
                "    # Fallback for demonstration if user hasn't generated data yet\n",
                "    print(\"Error: File not found.\")\n",
                "    df = pd.DataFrame()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Downsample to 50K (Stratified to maintain Fraud ratio or simple random)\n",
                "# User requested downsampling to 50K. We'll use simple random sampling but ensure we don't lose all frauds if they are rare.\n",
                "# Actually, for fraud detection, usually we want to keep ALL frauds and downsample ONLY non-frauds.\n",
                "# But for simplicity and strict adherence to \"Downsample to 50K transactions\", we will sample 50k random rows.\n",
                "\n",
                "if len(df) > DOWNSAMPLE_SIZE:\n",
                "    # Stratified sampling to preserve class distribution\n",
                "    df = df.groupby('Class', group_keys=False).apply(lambda x: x.sample(int(np.rint(DOWNSAMPLE_SIZE*len(x)/len(df))))).sample(frac=1).reset_index(drop=True)\n",
                "    print(f\"Downsampled to {len(df)} rows.\")\n",
                "else:\n",
                "    print(\"Dataset smaller than target, using all rows.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Feature Engineering\n",
                "\n",
                "# Log Scale Amount\n",
                "df['Amount_log'] = np.log1p(df['Amount'])\n",
                "\n",
                "# Transaction Frequency per User (Velocity)\n",
                "# Note: The original Kaggle dataset implies PCA features only, but we added a synthetic 'UserID' in our generation script\n",
                "# or we assume the user has a dataset with UserID. If 'UserID' is missing, we create a dummy one for the sake of the logic.\n",
                "if 'UserID' not in df.columns:\n",
                "    print(\"UserID column missing. Generating dummy UserIDs for feature engineering demonstration.\")\n",
                "    df['UserID'] = np.random.randint(1, 5000, df.shape[0])\n",
                "\n",
                "# Calculate Velocity: Transactions per hour (or just count per user as proxy for frequency)\n",
                "# A more robust way: Count transactions by this user (overall or in a window)\n",
                "df['Txn_per_user'] = df.groupby('UserID')['Time'].transform('count')\n",
                "\n",
                "# Example High Risk Flag: High amount & High frequency\n",
                "df['High_Risk_Flag'] = ((df['Amount_log'] > df['Amount_log'].quantile(0.95)) & \n",
                "                        (df['Txn_per_user'] > df['Txn_per_user'].quantile(0.95))).astype(int)\n",
                "\n",
                "print(\"Feature Engineering Complete.\")\n",
                "print(df[['Amount', 'Amount_log', 'UserID', 'Txn_per_user', 'High_Risk_Flag']].head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Export\n",
                "df.to_csv(OUTPUT_FILE, index=False)\n",
                "print(f\"Cleaned data saved to {OUTPUT_FILE}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}